{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model to forecast inventory demand based on historical sales data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model accuracy is RMSLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmsle(y, y0):\n",
    "    assert len(y) == len(y0)\n",
    "    return np.sqrt(np.mean(np.power(np.log1p(y)-np.log1p(y0), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data \n",
    "The size of the training data is quite large (~4 GB). Large datasets require significant amount of memory to process. Instead, we will sample the data randomly for our initial data analysis and visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_samp_data(filename='train.csv', columns=[], load_pkl=1):\n",
    "    \"\"\" \n",
    "      Function returns a dataframe containing the training data sampled randomly. \n",
    "      The data is also stored in a pickle file for later processing.\n",
    "    \"\"\"\n",
    "    if load_pkl:\n",
    "        inputfile = open('train_samp_data.pkl', 'rb')\n",
    "        data = pickle.load(inputfile)\n",
    "        inputfile.close()\n",
    "        return data\n",
    "    \n",
    "    chunksize= 10 ** 6\n",
    "    datasize = 74180464 #datasize = sum(1 for line in open(filename)) - 1 #number of records in file (excludes header)\n",
    "    samplesize = 3*10 ** 4 # samples per chunk of data read from the file.\n",
    "    \n",
    "    data = pd.DataFrame([],columns=columns)\n",
    "    chunks = pd.read_csv(filename, iterator=True, chunksize=chunksize)\n",
    "    for chunk in chunks:\n",
    "        chunk.columns = columns\n",
    "        data = data.append(chunk.sample(samplesize)) \n",
    "    \n",
    "    # write data to a pickle file.\n",
    "    outputfile = open('train_samp_data.pkl','wb')\n",
    "    pickle.dump(data,outputfile)\n",
    "    outputfile.close()\n",
    "    \n",
    "    return data\n",
    " \n",
    "load_pkl = 0\n",
    "columns = ['week_num', 'sales_depot_id', 'sales_chan_id', 'route_id', 'client_id', 'prod_id', 'saleunit_curr_wk', 'saleamt_curr_wk', 'retunit_next_week', 'retamt_next_wk', 'y_pred_demand']\n",
    "tic = time.time()\n",
    "train_data_samp = load_samp_data('train.csv', columns, load_pkl)\n",
    "toc = time.time()\n",
    "print '**'\n",
    "print 'Time to load: ', toc-tic, 'sec'\n",
    "print \n",
    "print train_data_samp.describe()\n",
    "print '**'\n",
    "print train_data_samp[['week_num', 'sales_depot_id', 'sales_chan_id', 'route_id', 'client_id', 'prod_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary evaluation using Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.cross_validation import train_test_split\n",
    "features_train = train_data_samp[['week_num', 'sales_depot_id', 'sales_chan_id', 'route_id', 'client_id', 'prod_id']].values\n",
    "labels_train = train_data_samp[['y_pred_demand']].values\n",
    "\n",
    "# Split the data samples into train and test.\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_train, labels_train, test_size=0.33, random_state=42)\n",
    "\n",
    "# Linear regression\n",
    "tic = time.time()\n",
    "clf = linear_model.LinearRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "pred[pred<0] = 0\n",
    "tac = time.time()\n",
    "print '----------'\n",
    "print 'Time:', tac-tic, 'RMSLE (LinearRegression):', rmsle(pred, y_test)\n",
    "print '----------'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary evaluation using gradient boosting (xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function to report best scores\n",
    "def report(grid_scores, n_top=3):\n",
    "    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"Model with rank: {0}\".format(i + 1))\n",
    "        print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "              score.mean_validation_score,\n",
    "              np.std(score.cv_validation_scores)))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.00601291656494 RMSLE (LinearRegression): 0.935833411032\n",
      "----------\n",
      "RandomizedSearchCV took 1190.15 seconds for 20 candidates parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.237 (std: 0.003)\n",
      "Parameters: {'bootstrap': False, 'min_samples_leaf': 3, 'min_samples_split': 5, 'criterion': 'entropy', 'max_features': 5, 'max_depth': 10}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.236 (std: 0.002)\n",
      "Parameters: {'bootstrap': False, 'min_samples_leaf': 2, 'min_samples_split': 2, 'criterion': 'entropy', 'max_features': 5, 'max_depth': 10}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.236 (std: 0.003)\n",
      "Parameters: {'bootstrap': True, 'min_samples_leaf': 5, 'min_samples_split': 3, 'criterion': 'gini', 'max_features': 5, 'max_depth': 10}\n",
      "\n",
      "Time: 0.00601291656494 RMSLE (RF): 0.786554235216\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "\n",
    "from scipy.stats import randint as sp_randint\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=30)\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {\"max_depth\": [10, None],\n",
    "              \"max_features\": sp_randint(1, 6),\n",
    "              \"min_samples_split\": sp_randint(1, 6),\n",
    "              \"min_samples_leaf\": sp_randint(1, 6),\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 20\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search, n_jobs=2)\n",
    "start = time.time()\n",
    "random_search.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time.time() - start), n_iter_search))\n",
    "report(random_search.grid_scores_)\n",
    "random_search.best_score_ \n",
    "pred = random_search.predict(X_test)\n",
    "pred[pred<0] = 0\n",
    "print 'Time:', tac-tic, 'RMSLE (RF):', rmsle(pred, np.ravel(y_test))\n",
    "print '----------'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clientnameid_data = pd.read_csv('cliente_tabla.csv')\n",
    "townstate_data = pd.read_csv('town_state.csv')\n",
    "print clientnameid_data.head()\n",
    "print '----'\n",
    "print townstate_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "There are duplicate client ids in cliente_table, which means one client id may have multiple client name that are very similar. We will cluster them based on a hash function and use a clustering algorithm to evaluate similarity.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def hash_eval(s):\n",
    "    hash_base = 4\n",
    "    s = re.sub('[., ]', '', s)\n",
    "    seqlen = len(s)\n",
    "    n = seqlen - 1\n",
    "    h = 0\n",
    "    for c in s:\n",
    "        h += ord(c) * (hash_base ** n)\n",
    "        n -= 1\n",
    "    curhash = h\n",
    "    return curhash\n",
    "\n",
    "# In the client table, same clients are assigned different client ID. We create a new client table where clients are assigned unique ID. \n",
    "clientid_hash = dict()\n",
    "new_client_id = [-1]   \n",
    "for idx, s in enumerate(clientnameid_data.NombreCliente):\n",
    "    t = hash_eval(s)\n",
    "    clientid_hash.setdefault(t, []).append(clientnameid_data.Cliente_ID[idx])\n",
    "    if t in clientid_hash:\n",
    "        a = clientid_hash[t]\n",
    "        new_client_id.append(a[0])\n",
    "\n",
    "# In the agency table, same agencies (town, state) are assigned different agency ID. We create a new agency table where agencies (town, state) are assigned unique ID. \n",
    "agencyid_hash = dict()\n",
    "new_agency_id = [-1]   \n",
    "for idx, s in enumerate(townstate_data.Town+townstate_data.State):\n",
    "    t = hash_eval(s)\n",
    "    agencyid_hash.setdefault(t, []).append(townstate_data.Agencia_ID[idx])\n",
    "    if t in agencyid_hash:\n",
    "        a = agencyid_hash[t]\n",
    "        new_agency_id.append(a[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clientnameid_data['New_Cliente_ID'] = new_client_id[1:]\n",
    "townstate_data['New_Agencia_ID'] = new_agency_id[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print clientnameid_data.head(10)\n",
    "print '---'\n",
    "print townstate_data.head()\n",
    "print '---'\n",
    "print train_data_samp.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print train_data_samp.head(10)\n",
    "print '------'\n",
    "for idx, cid in enumerate(train_data_samp.client_id):\n",
    "    train_data_samp.client_id.values[idx] = clientnameid_data.New_Cliente_ID[train_data_samp.client_id.values[idx] == clientnameid_data.Cliente_ID.values].values[0]\n",
    "    train_data_samp.sales_depot_id.values[idx] = townstate_data.New_Agencia_ID[train_data_samp.sales_depot_id.values[idx] == townstate_data.Agencia_ID.values].values[0]\n",
    "print '-----'\n",
    "print train_data_samp.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test.csv')\n",
    "test_data.columns = ['id', 'week_num', 'sales_depot_id', 'sales_chan_id', 'route_id', 'client id', 'prod_id']\n",
    "test_labels = pd.read_csv('sample_submission.csv')\n",
    "test_data = test_data.drop('id', 1)\n",
    "print test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = sns.PairGrid(data_t)\n",
    "g.map(plt.scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = [[1, 2, 3, 4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.array(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.array(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.array(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print a.reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

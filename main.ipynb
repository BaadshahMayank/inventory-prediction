{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model to accurately forecast inventory demand based on historical sales data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data \n",
    "The size of the training data is quite large (~4 GB). Large datasets require significant amount of memory to process. Instead, we will sample the data randomly for our initial data analysis and visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "Time to load:  0.146552801132 sec\n",
      "\n",
      "           week_num  sales_depot_id  sales_chan_id      route_id  \\\n",
      "count  75000.000000    75000.000000   75000.000000  75000.000000   \n",
      "mean       5.983653     2776.219267       1.377787   2111.400187   \n",
      "std        2.027215     4644.927230       1.450673   1480.490670   \n",
      "min        3.000000     1110.000000       1.000000      1.000000   \n",
      "25%        4.000000     1312.000000       1.000000   1161.000000   \n",
      "50%        6.000000     1614.000000       1.000000   1285.000000   \n",
      "75%        8.000000     2038.000000       1.000000   2802.000000   \n",
      "max        9.000000    25759.000000      11.000000   9319.000000   \n",
      "\n",
      "          client_id       prod_id  saleunit_curr_wk  saleamt_curr_wk  \\\n",
      "count  7.500000e+04  75000.000000      75000.000000     75000.000000   \n",
      "mean   1.795300e+06  20893.475467          7.204680        67.336995   \n",
      "std    1.832930e+06  18660.676981         21.827121       290.845496   \n",
      "min    1.060000e+02     72.000000          0.000000         0.000000   \n",
      "25%    3.569272e+05   1242.000000          2.000000        16.760000   \n",
      "50%    1.192904e+06  30549.000000          3.000000        30.000000   \n",
      "75%    2.367064e+06  37519.000000          6.000000        55.840000   \n",
      "max    1.169326e+07  49986.000000       2275.000000     22159.270000   \n",
      "\n",
      "       retunit_next_week  retamt_next_wk  y_pred_demand  \n",
      "count       75000.000000    75000.000000   75000.000000  \n",
      "mean            0.117200        1.161824       7.126373  \n",
      "std             1.810382       16.745748      21.717600  \n",
      "min             0.000000        0.000000       0.000000  \n",
      "25%             0.000000        0.000000       2.000000  \n",
      "50%             0.000000        0.000000       3.000000  \n",
      "75%             0.000000        0.000000       6.000000  \n",
      "max           231.000000     1828.760000    2275.000000  \n",
      "**\n"
     ]
    }
   ],
   "source": [
    "def load_samp_data(filename='train.csv', columns=[], load_pkl=1):\n",
    "    \"\"\" \n",
    "      Function returns a dataframe containing the training data sampled randomly. \n",
    "      The data is also stored in a pickle file for later processing.\n",
    "    \"\"\"\n",
    "    if load_pkl:\n",
    "        inputfile = open('train_samp_data.pkl', 'rb')\n",
    "        data = pickle.load(inputfile)\n",
    "        inputfile.close()\n",
    "        return data\n",
    "    \n",
    "    chunksize= 10 ** 6\n",
    "    datasize = 74180464 #datasize = sum(1 for line in open(filename)) - 1 #number of records in file (excludes header)\n",
    "    samplesize = 10 ** 3 # samples per chunk of data read from the file.\n",
    "    \n",
    "    data = pd.DataFrame([],columns=columns)\n",
    "    chunks = pd.read_csv(filename, iterator=True, chunksize=chunksize)\n",
    "    for chunk in chunks:\n",
    "        chunk.columns = columns\n",
    "        data = data.append(chunk.sample(samplesize)) \n",
    "    \n",
    "    # write data to a pickle file.\n",
    "    outputfile = open('train_samp_data.pkl','wb')\n",
    "    pickle.dump(data,outputfile)\n",
    "    outputfile.close()\n",
    "    \n",
    "    return data\n",
    " \n",
    "load_pkl = 1\n",
    "columns = ['week_num', 'sales_depot_id', 'sales_chan_id', 'route_id', 'client_id', 'prod_id', 'saleunit_curr_wk', 'saleamt_curr_wk', 'retunit_next_week', 'retamt_next_wk', 'y_pred_demand']\n",
    "tic = time.time()\n",
    "train_data_samp = load_samp_data('train.csv', columns, load_pkl)\n",
    "toc = time.time()\n",
    "print '**'\n",
    "print 'Time to load: ', toc-tic, 'sec'\n",
    "print \n",
    "print train_data_samp.describe()\n",
    "print '**'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cliente_ID</th>\n",
       "      <th>NombreCliente</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>SIN NOMBRE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>OXXO XINANTECATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>SIN NOMBRE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>EL MORENO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>SDN SER  DE ALIM  CUERPO SA CIA  DE INT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cliente_ID                            NombreCliente\n",
       "0           0                               SIN NOMBRE\n",
       "1           1                         OXXO XINANTECATL\n",
       "2           2                               SIN NOMBRE\n",
       "3           3                                EL MORENO\n",
       "4           4  SDN SER  DE ALIM  CUERPO SA CIA  DE INT"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clientnameid_data = pd.read_csv('cliente_tabla.csv')\n",
    "clientnameid_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "There are duplicate client ids in cliente_table, which means one client id may have multiple client name that are very similar. We will cluster them based on a hash function and use a clustering algorithm to evaluate similarity.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hash_eval(s):\n",
    "    hash_base = 4\n",
    "    s = \"\".join(s.split())\n",
    "    seqlen = len(s)\n",
    "    n = seqlen - 1\n",
    "    h = 0\n",
    "    for c in s:\n",
    "        h += ord(c) * (hash_base ** n)\n",
    "        n -= 1\n",
    "    curhash = h\n",
    "    return curhash\n",
    "\n",
    "clientid_hash = dict()\n",
    "new_client_id = [-1]   \n",
    "for idx, s in enumerate(clientnameid_data.NombreCliente):\n",
    "    t = hash_eval(s)\n",
    "    clientid_hash.setdefault(hash_eval(s), []).append(clientnameid_data.Cliente_ID[idx])\n",
    "    if t in clientid_hash:\n",
    "        a = clientid_hash[t]\n",
    "        new_client_id.append(a[0])\n",
    "    else: \n",
    "        new_client_id.append(np.max(new_client_id)+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cliente_ID       935362\n",
      "NombreCliente    935362\n",
      "dtype: int64 935362\n"
     ]
    }
   ],
   "source": [
    "clientnameid_data['New_Cliente_ID'] = new_client_id[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cliente_ID</th>\n",
       "      <th>NombreCliente</th>\n",
       "      <th>New_Cliente_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>SIN NOMBRE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>OXXO XINANTECATL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>SIN NOMBRE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>EL MORENO</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>SDN SER  DE ALIM  CUERPO SA CIA  DE INT</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>SDN SER DE ALIM CUERPO SA CIA DE INT</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>LA VAQUITA</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>LUPITA</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>I M EL GUERO</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>MINI SUPER LOS LUPES</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cliente_ID                            NombreCliente  New_Cliente_ID\n",
       "0           0                               SIN NOMBRE               0\n",
       "1           1                         OXXO XINANTECATL               1\n",
       "2           2                               SIN NOMBRE               0\n",
       "3           3                                EL MORENO               3\n",
       "4           4  SDN SER  DE ALIM  CUERPO SA CIA  DE INT               4\n",
       "5           4     SDN SER DE ALIM CUERPO SA CIA DE INT               4\n",
       "6           5                               LA VAQUITA               5\n",
       "7           6                                   LUPITA               6\n",
       "8           7                             I M EL GUERO               7\n",
       "9           8                     MINI SUPER LOS LUPES               8"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clientnameid_data.head(10)\n",
    "\n",
    "#print len(clientid_hash), ' ', len(np.unique(np.array(clientid_hash)))\n",
    "#sns.jointplot(np.arange(len(new_client_ID)), np.array(new_client_ID))\n",
    "#sns.jointplot(np.arange([len(new_client_ID)]), np.array(new_client_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large datasets require significant amount of memory. Instead of loading the entire training data into memory, we use a subset of training data which is sampled uniformly at random for our initial data analysis and visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "startt = time.time()\n",
    "datasize = sum(1 for line in open(filename)) - 1 #number of records in file (excludes header)\n",
    "endt = time.time()\n",
    "print endt-startt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasize = 74180464\n",
    "samplesize = 5000\n",
    "\n",
    "\n",
    "data_train.columns = ['week_num', 'sales_depot_id', 'sales_chan_id', 'route_id', 'client id', 'prod_id', 'saleunit_curr_wk', 'saleamt_curr_wk', 'retunit_next_week', 'retamt_next_wk', 'y_pred_demand']\n",
    "data_test = pd.read_csv('test.csv', nrows=100)\n",
    "data_test.columns = ['id', 'week_num', 'sales_depot_id', 'sales_chan_id', 'route_id', 'client id', 'prod_id']\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = sns.PairGrid(data_t)\n",
    "g.map(plt.scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chunksize= 10 ** 6\n",
    "datasize = 0\n",
    "startt = time.time()\n",
    "for chunk in pd.read_csv(filename, chunksize=chunksize):\n",
    "    datasize = datasize + len(chunk)\n",
    "    \n",
    "\n",
    "    \n",
    "endt = time.time()\n",
    "del chunk\n",
    "print 'time: ', endt-startt, '(sec), datasize: ', datasize\n",
    "\n",
    "datasize = sum(1 for line in open(filename)) - 1 #number of records in file (excludes header)\n",
    "s = 10000 #desired sample size\n",
    "skip = sorted(random.sample(xrange(1,n+1),n-s)) #the 0-indexed header will not be included in the skip list\n",
    "df = pandas.read_csv(filename, skiprows=skip)\n",
    "\n",
    "\n",
    "#sns.distplot(train_data_samp.client_id)\n",
    "#sns.stripplot(x=\"week_num\", y=\"saleunit_curr_wk\", data=train_data_samp, jitter=True);\n",
    "#sns.jointplot(x='sales_depot_id', y='saleunit_curr_wk', data=train_data_samp)\n",
    "#sns.jointplot(x='prod_id', y='saleunit_curr_wk', data=train_data_samp)\n",
    "#g = sns.PairGrid(train_data_samp)\n",
    "#g.map(plt.scatter)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Feature Analysis: Investigate the feature vector space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
